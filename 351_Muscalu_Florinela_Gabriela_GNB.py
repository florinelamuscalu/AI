# -*- coding: utf-8 -*-
"""351_Muscalu_Florinela_Gabriela.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iatrALbesIqgH7Th6US3A-Q03jf96BWa
"""

!pip install pyphen

import nltk
nltk.download('punkt')
nltk.download('wordnet')

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import balanced_accuracy_score
import pyphen
import numpy as np
import pandas as pd
from nltk.corpus import wordnet
from dale_chall import DALE_CHALL
from nltk.tokenize import word_tokenize

dtypes = {"sentence": "string", "token": "string", "complexity": "float64"}
train = pd.read_excel('train.xlsx', dtype=dtypes, keep_default_na=False)
test = pd.read_excel('test.xlsx', dtype=dtypes, keep_default_na=False)

def corpus_feature(corpus):
  if corpus =='bible':
    return [0]
  elif corpus =='biomed':
    return [1]
  else:
    return [2]

from compound_word import compound_word

def is_compound_word(word):
  return int(word.lower() in compound_word)

def get_all_tokens(df):
  all_words=[]
  for _,item in df.iterrows():
    tokens=word_tokenize(item['sentence'])
    for t in tokens:
      all_words.append(t)
  
  
  with open('all_words.txt', 'w', encoding="utf-8") as f: 
    for i in range(len(all_words)):
      f.write(str(all_words[i]) + '\n' )
  return all_words

get_all_tokens(train)

def frequency_word(word_dat):
  contor = 0
  nr_word = 0
  with open('all_words.txt', 'r', encoding = 'utf-8') as file_word: 
    word = file_word.readlines()
    for one_word in word:
      one_word = one_word.strip()
      #print(word_dat,one_word)
      if word_dat == one_word:
        #print(one_word)
        contor += 1
      nr_word += 1
    average = contor/nr_word
    #print(nr_word)
    #print(contor)
    return average

def get_word_structure_features(word):
    features = []
    features.append(is_compound_word(word))
    features.append(frequency_word(word))
    return np.array(features)

def synsets(word):
  print(wordnet.synsets(word))
  return len(wordnet.synsets(word))

def hyponyms(word):
  suma =0
  avrage =0
  for sys in wordnet.synsets(word):
    suma += len(sys.hyponyms())
  if len(wordnet.synsets(word)) !=0 :
    avrage = suma / len(wordnet.synsets(word))
  return avrage

def hypernyms (word):
  suma =0
  avrage =0
  for sys in wordnet.synsets(word):
   suma += len(sys.hypernyms())
  if len(wordnet.synsets(word)) !=0 :
    avrage = suma / len(wordnet.synsets(word))
  return avrage

def part_holonyms(word):
  suma =0
  avrage =0
  for sys in wordnet.synsets(word):
    suma += len(sys.part_holonyms())
  if len(wordnet.synsets(word)) !=0 :
    avrage = suma / len(wordnet.synsets(word))
  return avrage

def get_wordnet_features(word):
  features = []
  features.append(synsets(word)) 
  features.append(hyponyms(word))
  features.append(hypernyms(word))
  features.append(part_holonyms(word))
  return np.array(features)

def featurize(row):
    word = row['token']
    all_features = []
    all_features.extend(corpus_feature(row['corpus']))
    all_features.extend(get_word_structure_features(word))
    all_features.extend(get_wordnet_features(word))
    return np.array(all_features)

def featurize_df(df):
    nr_of_features = len(featurize(df.iloc[0]))
    nr_of_examples = len(df)
    features = np.zeros((nr_of_examples, nr_of_features))
    for index, row in df.iterrows():
        row_ftrs = featurize(row)
        features[index, :] = row_ftrs
    return features

X_train = featurize_df(train)
y_train = train['complex'].values

X_test = featurize_df(test)

from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
y_pred = gnb.fit(X_train, y_train).predict(X_test)

test_id = np.arange(7663,9001)

np.savetxt("Naive_Bayes_Gaussian_version8.csv",np.stack((test_id,y_pred)).T,fmt="%d",delimiter=',',header="id,complex",comments="")